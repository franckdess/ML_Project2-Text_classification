MACHINE LEARNING - PROJECT 2
REPORT


Abstract

Text classification is a core topic of Machine Learning. It has a broad range of applications such as sentiment analysis of social media posts, email spam detection or articles classification.
Using Natural Language Processing, ...

"is used in every day life to determine sentiment or opinion from social media posts, detect spam emails, categorize articles or documents and many more. Based on Natural Language Processing, one can ask ourselves whether computers can understand a human language or not, and if not, try to figure it out what it is able do understand and how it can do it. Social media posts classification could be a main topic since a post, say a tweet"


Introduction

In this paper we are going to investigate which tools can can be used in order to allow a computer to understand the english language. More precisely, given a set of tweets, we we will train a model to enable a computer to decide wheter a given tweet reflects a positive or negative sentiment.

The data we are given are tweets which originally ended with either a :) smiley for positive tweets or a :( smiley for negative tweets and predict their sentiment from which the smileys where removed.

To train our model, we were given a set with only positive sentiment tweets and a set with only negative sentiment tweets.

We used the following specialized libraries in order to get the best accuracy possible :
- Pandas, to manipulate and analyze the data.
- Scikit-learn, providing us :
  - Classification functions
  - Validation functions, such as grid search with k-fold cross-validation, to optimize the model's hyperparameters.
  - Preprocessing functions, such as TfidfVectorizer, used to tranform tweets into vectors. 
- NLTK, to work with human language data.


1. How to transform words to vectors

A critical point of text classification is the transformation of words or sequence of words into vectors.
Assume we can map k sequences of words to k vectors of dimension N, with N being the number of features. Then we can naturally construct a matrix of dimension KxN.
Furthermore, assume that we know the sentiment of each sequence of words. Denote by 1 a positive sentiment and by -1 the negative sentiment.
We can then feed the KxN matrix along with the list of sentiments to a classifier, which will output a model.

First Model

We started with the following simple model to get acquainted with the data. As a first step, we mapped every word into a vector using the GloVe algorithm. The GloVe algorithm is a variant of Word2Vec that allows us to obtain vector representations for words. Training is performed using a co-occurence matrix, with entries corresponding to the number of times a specific word occurs together with a context word.

Now that we have a vector representation for each word, we need to obtain the vector representation for each tweet.
Our first approach to obtain the vector representation of a tweet was to take the average of the vector representation of the words constituing this tweet.

The vector representation of each 

Now we have the vector for each tweet; this is our train features and we have the sentiment of each of these tweets, this is the train prediction. Having both of them we ran many classifiers to obtain a model. 
------------------------------------------------------------------------------------------------
COMPARAISON OF STANDARD CLASSIFIERS
SGDCLASSFIER, LOGISTICREGRESSION, LINEARSVC
------------------------------------------------------------------------------------------------
Clearly by looking at the above graphs we can see that the accuracy of our model is bounded by XXX. This means that we have to improve our preprocessing step.

2. Taking into account the weight of each word into the tweet

By looking at our first model, we saw one step that could be improved, which is the way of forming the tweet's vector. Indeed in the last model, we simply took the average over each word for the tweet. This is not a proper way to form the vector since it does not take into account the weight of the words. This is a quit simple concept. A word like 'a', 'the', 'this' and so on do not bring the same amount of information as 'happy' or 'sad' for example. Hence weighting each word would be a great move forward in order to obtain a better accuracy. This is done by using the TfidfVectorizer function of the library Scikit learn.
The TfidfVectorizer function converts a collection of raw documents to a matrix of TF-IDF features. Now let us explain simply what is a TF-IFD matrix. Assume one want to add weight to word. A simple way to do that is by taking the frequency of the word in a given document. It is, if a word appears many times in a document (an article, a book, ...) this word is important for that document. But, in the other hand, if this word appears often in a given document, but also appears often in other documents, this word is no longer important. This is the case for 'a', 'the', and so on. These words appear often in all document. Therefore the weight of these words should be really small since it does not bring any relevant information.

Back to our TfidfVectorizer, this function will take a list of documents and vectorize them, by taking into account the weight of each word, that is its TF-IDF 'score'. The list of document is no other than the list of tweets. Each tweet can be seen as a document. Hence TfidfVectorizer directly compute the vector of a tweet instead of computing the vector a word. We have now a second set of vectorized tweets. We will apply the same reasoning as above by showing the difference of accuracies between many classifiers.
------------------------------------------------------------------------------------------------
RUN TF-IDF WITH NO PARAMETERS AND PRINT ACCURACIES FOR DIFFERENT CLASSIFIERS WITH NO PARAMETERS
SGDCLASSIFIER, LOGISTICREGRESSION, LINEARSVC
------------------------------------------------------------------------------------------------

Note that the TfidfVectorizer return a matrix where each row corresponds to the vector of a tweet. The matrix returned by TfidfVectorizer is a sparse matrix. This prevent us to make much transformations on it but it allow the computation to be way more fast.

3. Improve TfidfVectorizer by taking into account that we are working with tweets

The last model described was a pretty good improvement compared to the first one. But something can still be improved. Indeed the TfidfVectorizer function can be customized. The stop words list can be tuned as well as the tokenizer. We first get into the stop words. In the last model the stop words used were the default one of NLTK, that is the stop words of the english language. But the tweets in general do not follow the usual principles of grammar and vocabulary. That is, we needed to find a new list of stop words that would better fit the context of social media. By searching on the web we found many lists of stop word related to social media. This stop word would certainly increase the accuracy since it takes into account that we are working with tweets and indeed it is a list of stop words for tweet. We combined 3 lists that can be found HERE in order to obtain the stop words list twitter-stopwords-final.txt.

Another parameter of the TfidfVectorizer that is tunable is the tokenizer. The tokenizer is the function that transforms a tweet into a list of words. TfidfVectorizer has a default tokenizer, but once again it does not take into account the fact that we are on a social media context. To settle this issue, we define our own tuned tokenizer:

	def tokenize(t):
    		tweet_tok = TweetTokenizer(strip_handles=True, reduce_len=True)
    		tokens = tweet_tok.tokenize(t)
    		wnl = WordNetLemmatizer()
    		stems = []
    		for item in tokens:
        		stems.append(wnl.lemmatize(item))
    		return stems

The TweetTokenizer() is a tweet-aware tokenizer, i.e, a function that splits the tweet into words knowing that it is working with tweets. This means that it takes into account smiley like ':-)' or hashtags.
After the tokenizing step, we pass each token into a lemmatizer function. The lemmatizing step is the lexical analysis, which group words of the same family. This means that each word will be taken back to its root. It groups the different forms a word can take: a name, a plural a verb an infinitive and so on. For example, 'taking' will be taken back to 'take', 'dogs' will be taken back to 'dog'.

Now we can construct the new TfidfVectorizer the following way :

	vectorizer = TfidfVectorizer(analyzer='word', stop_words = stopw, tokenizer=tokenize, ngram_range=(1,4))

Having an updated version of the tweets' vectors, we ran the same classifiers as above to check for improvements.

------------------------------------------------------------------------------------------------
RUN TF-IDF WITH NO PARAMETERS AND PRINT ACCURACIES FOR DIFFERENT CLASSIFIERS WITH NO PARAMETERS
SGDCLASSIFIER, LOGISTICREGRESSION, LINEARSVC
------------------------------------------------------------------------------------------------

We can clearly see an improvement compared to the previous versions of tweets' vectors.

4. Find the best classifier and tune its parameters







