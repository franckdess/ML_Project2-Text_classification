MACHINE LEARNING - PROJECT 2
REPORT


Abstract

Text classification is a core topic of Machine Learning. It has a broad range of applications such as sentiment analysis of social media posts, email spam detection or articles classification.
Using Natural Language Processing, ...

"is used in every day life to determine sentiment or opinion from social media posts, detect spam emails, categorize articles or documents and many more. Based on Natural Language Processing, one can ask ourselves whether computers can understand a human language or not, and if not, try to figure it out what it is able do understand and how it can do it. Social media posts classification could be a main topic since a post, say a tweet"


Introduction

In this paper we are going to investigate which tools can can be used in order to allow a computer to understand the english language. More precisely, given a set of tweets, we we will train a model to enable a computer to decide wheter a given tweet reflects a positive or negative sentiment.

The data we are given are tweets which originally ended with either a :) smiley for positive tweets or a :( smiley for negative tweets and predict their sentiment from which the smileys where removed.

To train our model, we were given a set with only positive sentiment tweets and a set with only negative sentiment tweets.

We used the following specialized libraries in order to get the best accuracy possible :
- Pandas, to manipulate and analyze the data.
- Scikit-learn, providing us :
  - Classification functions
  - Validation functions, such as grid search with k-fold cross-validation, to optimize the model's hyperparameters.
  - Preprocessing functions, such as TfidfVectorizer, used to tranform tweets into vectors. 
- NLTK, to work with human language data.


1. How to transform words to vectors

A critical point of text classification is the transformation of words or sequence of words into vectors.
Assume we can map k sequences of words to k vectors of dimension N, with N being the number of features. Then we can naturally construct a matrix of dimension KxN.
Furthermore, assume that we know the sentiment of each sequence of words. Denote by 1 a positive sentiment and by -1 the negative sentiment.
We can then feed the KxN matrix along with the list of sentiments to a classifier, which will output a model.

First Model

We started with the following simple model to get acquainted with the data. As a first step, we mapped every word into a vector using the GloVe algorithm. The GloVe algorithm is a variant of Word2Vec that allows us to obtain vector representations for words. Training is performed using a co-occurence matrix, with entries corresponding to the number of times a specific word occurs together with a context word.

Now that we have a vector representation for each word, we need to obtain the vector representation for each tweet.
Our first approach to obtain the vector representation of a tweet was to take the average of the vector representation of the words constituing this tweet.

The vector representation of each tweet corresponds to our training features while the actual sentiment of the tweet corresponds to the train prediction.
Having both the features and the predictions we ran many different classifiers to obtain the best model possible.

------------------------------------------------------------------------------------------------
COMPARAISON OF STANDARD CLASSIFIERS
SGDCLASSFIER, LOGISTICREGRESSION, LINEARSVC
------------------------------------------------------------------------------------------------

By observing the above graphs, we can see that the accuracy of our model is bounded by XXX.
This means that we have to improve our preprocessing step in order to get a bettwer accuracy.

2. Taking into account the weight of each word into the tweet

The main aspect of our first model we can improve is our strategy to construct the vector representation of the tweet based on the vector representation of its individual words.

In our first model, we simply took the unweighted average of the vector representation of each word constituing the tweet. This is not an optimal way to form the vector since we do not take into account the importance of the words. For example stopwords such as 'a', 'the' or 'this' don't bring the same amount of information as 'happy' or 'sad', so they shouldn't be treated as having the same importance. It is therefore natural that attributing weights to words depending on their importance would result in a greater accuracy. We implement this improvement using the TfidfVectorizer function of the Scikit learn library.

The TfidfVectorizer function converts a collection of raw documents to a matrix of TF-IDF features. Now let us explain simply what is a TF-IFD matrix.

Let's now see what the TF-IDF matrix does.
Our aim is to give weights to the words according to their importance. A natural way to get the importance of a word is to take its frequency in a given document (an article, a book, a tweet, ...). But if a word also appears often in other documents, the importance of this word is reduced. Take stopwords for example, they appear in all documents and therefore should have a very small weight as they don't bring any relevant information.


Back to our TfidfVectorizer, this function will take a list of documents and vectorize them, by taking into account the weight of each word, that is its TF-IDF 'score'. The list of document is no other than the list of tweets. Each tweet can be seen as a document. Hence TfidfVectorizer directly compute the vector of a tweet instead of computing the vector a word. We have now a second set of vectorized tweets. We will apply the same reasoning as above by showing the difference of accuracies between many classifiers.
------------------------------------------------------------------------------------------------
RUN TF-IDF WITH NO PARAMETERS AND PRINT ACCURACIES FOR DIFFERENT CLASSIFIERS WITH NO PARAMETERS
SGDCLASSIFIER, LOGISTICREGRESSION, LINEARSVC
------------------------------------------------------------------------------------------------

Note that the TfidfVectorizer return a matrix where each row corresponds to the vector of a tweet. The matrix returned by TfidfVectorizer is a sparse matrix. This prevent us to make much transformations on it but it allow the computation to be way more fast.

3. Improve TfidfVectorizer by taking into account that we are working with tweets

The last model described was a pretty good improvement compared to the first one. But something can still be improved. Indeed the TfidfVectorizer function can be customized. The stop words list can be tuned as well as the tokenizer. We first get into the stop words. In the last model the stop words used were the default one of NLTK, that is the stop words of the english language. But the tweets in general do not follow the usual principles of grammar and vocabulary. That is, we needed to find a new list of stop words that would better fit the context of social media. By searching on the web we found many lists of stop word related to social media. This stop word would certainly increase the accuracy since it takes into account that we are working with tweets and indeed it is a list of stop words for tweet. We combined 3 lists that can be found HERE in order to obtain the stop words list twitter-stopwords-final.txt.

Another parameter of the TfidfVectorizer that is tunable is the tokenizer. The tokenizer is the function that transforms a tweet into a list of words. TfidfVectorizer has a default tokenizer, but once again it does not take into account the fact that we are on a social media context. To settle this issue, we define our own tuned tokenizer:

	def tokenize(t):
    		tweet_tok = TweetTokenizer(strip_handles=True, reduce_len=True)
    		tokens = tweet_tok.tokenize(t)
    		wnl = WordNetLemmatizer()
    		stems = []
    		for item in tokens:
        		stems.append(wnl.lemmatize(item))
    		return stems

The TweetTokenizer() is a tweet-aware tokenizer, i.e, a function that splits the tweet into words knowing that it is working with tweets. This means that it takes into account smiley like ':-)' or hashtags.
After the tokenizing step, we pass each token into a lemmatizer function. The lemmatizing step is the lexical analysis, which group words of the same family. This means that each word will be taken back to its root. It groups the different forms a word can take: a name, a plural a verb an infinitive and so on. For example, 'taking' will be taken back to 'take', 'dogs' will be taken back to 'dog'.

Now we can construct the new TfidfVectorizer the following way :

	vectorizer = TfidfVectorizer(analyzer='word', stop_words = stopw, tokenizer=tokenize, ngram_range=(1,4))

Having an updated version of the tweets' vectors, we ran the same classifiers as above to check for improvements.

------------------------------------------------------------------------------------------------
RUN TF-IDF WITH NO PARAMETERS AND PRINT ACCURACIES FOR DIFFERENT CLASSIFIERS WITH NO PARAMETERS
SGDCLASSIFIER, LOGISTICREGRESSION, LINEARSVC
------------------------------------------------------------------------------------------------

We can clearly see an improvement compared to the previous versions of tweets' vectors.

4. Find the best classifier and tune its parameters







